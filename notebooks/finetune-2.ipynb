{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07b9a12",
   "metadata": {},
   "source": [
    "# **Political Concept Classifer Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcb307",
   "metadata": {},
   "source": [
    "### The Logic \n",
    "\n",
    "1. We will be using the sloth to finetune the model \n",
    "2. PEFT will allow us to add LoRA adapters which will allow us to finetune the model on a smaller dataset\n",
    "3. TRT will allow us to add the techinical configurations needed\n",
    "4. We will download the GUFF file and save it in the political_concept_classifer folder\n",
    "5. Upload the downloaded model to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a78be3",
   "metadata": {},
   "source": [
    "**You should run it on Google Colab**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f4603",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "We will use a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8609fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = json.load(open(\"training.json\", \"r\"))\n",
    "print(file[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81d371",
   "metadata": {},
   "source": [
    "### Install all dependencies\n",
    "Install them via pip (python package manager). Ensure python environment is set up properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y unsloth peft\n",
    "%pip install unsloth trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9553ad",
   "metadata": {},
   "source": [
    "### Load pretrained model (without fine tuning)\n",
    "We use the Phi 3 mini model from unsloth due to its low-weight and fast speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e6a18",
   "metadata": {},
   "source": [
    "### Prepare dataset for finetuning\n",
    "Ensure that the LLM knows what is expected input and output by specifying the format of the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec946e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    return f\"### Input: {example['text']}\\n### Output: {example['category']}<|endoftext|>\"\n",
    "\n",
    "formatted_data = [format_prompt(item) for item in file]\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb2501",
   "metadata": {},
   "source": [
    "### Add LoRA adapters \n",
    "LoRA adapters is a techinique to add a small amount of parameters to the model to improve its performance on a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # LoRA rank - higher = more capacity, more memory\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None, # LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89464a4d",
   "metadata": {},
   "source": [
    "### Add technical configurations to the model\n",
    "These are all the technical details needed to fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1732882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f284a",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "We can now fine tune model. This step takes the most time to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ad200",
   "metadata": {},
   "source": [
    "### Upload fine tuned model to Hugging Face\n",
    "The fine tuned model can now be used in transformer lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6210b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(\"hemitpatel/politigo\", tokenizer, save_method = \"merged_16bit\", token = \"hf_token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
